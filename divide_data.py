#!/usr/bin/env python3
# divide_data.py
"""
ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸

image_data.csv íŒŒì¼ì„ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
4ê°œ í´ë˜ìŠ¤(full_shot, detail_shot, neck_label, care_label)ë¡œ ë¶„ë¥˜í•˜ì—¬
train/validation/testë¡œ ë¶„í• í•˜ì—¬ data/ í´ë”ì— ì €ì¥

ì‚¬ìš©ë²•:
    python divide_data.py                    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¶„í• 
    python divide_data.py --train-ratio 0.7 # ì»¤ìŠ¤í…€ ë¹„ìœ¨
    python divide_data.py --random-state 123 # ì‹œë“œ ë³€ê²½
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from collections import Counter
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def expand_images_to_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì œí’ˆë³„ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ í™•ì¥
    
    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (ì œí’ˆë³„)
        
    Returns:
        í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ (ì´ë¯¸ì§€ë³„)
    """
    logger.info("ğŸ”„ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ ë°ì´í„° í™•ì¥ ì¤‘...")
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    expanded_rows = []
    
    # ì›ë³¸ CSV ì»¬ëŸ¼ â†’ image_type ë§¤í•‘
    # 4ê°œ í´ë˜ìŠ¤: full_shot, detail_shot, neck_label, care_label
    image_type_columns = {
        'main_image': 'full_shot',
        'back_image': 'full_shot',
        'text_tag_image': 'care_label',
        'neck_tag_image': 'neck_label',
        'other': 'detail_shot'
    }
    
    for idx, row in df.iterrows():
        product_id = row['id']
        
        # ê° ì´ë¯¸ì§€ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        for column_name, image_type in image_type_columns.items():
            if column_name not in df.columns:
                continue
                
            image_paths_str = row[column_name]
            
            # ë¹ˆ ê°’ì´ë©´ ê±´ë„ˆë›°ê¸°
            if pd.isna(image_paths_str) or image_paths_str == '':
                continue
            
            # ì´ë¯¸ì§€ ê²½ë¡œë“¤ ë¶„ë¦¬ (& ë˜ëŠ” , ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            if '&' in str(image_paths_str):
                image_paths = str(image_paths_str).split('&')
            elif ',' in str(image_paths_str):
                image_paths = str(image_paths_str).split(',')
            else:
                image_paths = [str(image_paths_str)]
            
            # ê° ì´ë¯¸ì§€ ê²½ë¡œì— ëŒ€í•´ í–‰ ìƒì„±
            for image_path in image_paths:
                image_path = image_path.strip()
                if image_path == '' or image_path == 'nan':
                    continue
                
                new_row = {
                    'product_id': product_id,
                    'image_path': image_path,
                    'image_type': image_type,
                    # ë¶„í•  ì‹œì—ë§Œ ì‚¬ìš©í•  ì„ì‹œ ì»¬ëŸ¼ (ìµœì¢… ì €ì¥ ì‹œ ì œê±°)
                    '_temp_category_name': row['category_name']
                }
                
                expanded_rows.append(new_row)
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    expanded_df = pd.DataFrame(expanded_rows)
    
    logger.info(f"í™•ì¥ ì™„ë£Œ:")
    logger.info(f"  ì›ë³¸: {len(df)}ê°œ ì œí’ˆ")
    logger.info(f"  í™•ì¥í›„: {len(expanded_df)}ê°œ ì´ë¯¸ì§€")
    
    # ì´ë¯¸ì§€ íƒ€ì…ë³„ ë¶„í¬
    type_distribution = expanded_df['image_type'].value_counts()
    logger.info(f"ì´ë¯¸ì§€ íƒ€ì…ë³„ ë¶„í¬:")
    for img_type, count in type_distribution.items():
        logger.info(f"  {img_type}: {count:,}ê°œ")
    
    return expanded_df

def analyze_expanded_data(df: pd.DataFrame) -> dict:
    """í™•ì¥ëœ ë°ì´í„° ë¶„ì„"""
    analysis = {
        'total_images': len(df),
        'total_products': df['product_id'].nunique(),
        'columns': list(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'image_type_distribution': df['image_type'].value_counts().to_dict(),
        'category_distribution': df['_temp_category_name'].value_counts().to_dict(),
        'categories': df['_temp_category_name'].unique().tolist(),
        'num_categories': df['_temp_category_name'].nunique()
    }
    
    return analysis

def stratified_split_by_product(df: pd.DataFrame, 
                               train_ratio: float = 0.8,
                               val_ratio: float = 0.1,
                               test_ratio: float = 0.1,
                               random_state: int = 42) -> tuple:
    """
    ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í•  (ê°™ì€ ì œí’ˆì˜ ì´ë¯¸ì§€ë“¤ì´ ë‹¤ë¥¸ ì„¸íŠ¸ì— ì„ì´ì§€ ì•Šë„ë¡)
    
    Args:
        df: í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ (ì´ë¯¸ì§€ë³„)
        train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
        val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        random_state: ëœë¤ ì‹œë“œ
        
    Returns:
        train_df, val_df, test_df
    """
    # ë¹„ìœ¨ ê²€ì¦
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio}")
    
    logger.info("ğŸ”„ ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í•  ì¤‘...")
    
    # ì œí’ˆë³„ ì§‘ê³„ (ì¹´í…Œê³ ë¦¬ ê¸°ì¤€)
    product_summary = df.groupby('product_id').agg({
        '_temp_category_name': 'first'  # ì œí’ˆì˜ ì¹´í…Œê³ ë¦¬
    }).reset_index()
    
    stratify_column = '_temp_category_name'
    
    logger.info(f"ì œí’ˆ ë¶„í¬:")
    logger.info(f"  ì „ì²´ ì œí’ˆ: {len(product_summary):,}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
    category_dist = product_summary['_temp_category_name'].value_counts()
    logger.info(f"ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in category_dist.items():
        logger.info(f"  {category}: {count:,}ê°œ")
    
    # stratified splitì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (train/val/test ê°ê° 1ê°œ ì´ìƒ í•„ìš”)
    min_samples_for_stratify = 3
    
    # ìƒ˜í”Œì´ ì¶©ë¶„í•œ ì¹´í…Œê³ ë¦¬ì™€ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¦¬
    sufficient_categories = category_dist[category_dist >= min_samples_for_stratify].index.tolist()
    insufficient_categories = category_dist[category_dist < min_samples_for_stratify].index.tolist()
    
    if insufficient_categories:
        logger.warning(f"âš ï¸ ìƒ˜í”Œì´ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ ({min_samples_for_stratify}ê°œ ë¯¸ë§Œ): {insufficient_categories}")
        logger.info("   ì´ ì¹´í…Œê³ ë¦¬ë“¤ì€ ëœë¤í•˜ê²Œ ë¶„í• ë©ë‹ˆë‹¤.")
    
    # ì¶©ë¶„í•œ ì¹´í…Œê³ ë¦¬ì˜ ì œí’ˆë“¤
    sufficient_products = product_summary[product_summary[stratify_column].isin(sufficient_categories)]
    # ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ì˜ ì œí’ˆë“¤
    insufficient_products = product_summary[product_summary[stratify_column].isin(insufficient_categories)]
    
    train_products_list = []
    val_products_list = []
    test_products_list = []
    
    # 1. ì¶©ë¶„í•œ ì¹´í…Œê³ ë¦¬: stratified split
    if len(sufficient_products) > 0:
        try:
            # 1ì°¨ ë¶„í• : train vs (val + test)
            train_suff, temp_suff = train_test_split(
                sufficient_products,
                test_size=(val_ratio + test_ratio),
                stratify=sufficient_products[stratify_column],
                random_state=random_state
            )
            
            # 2ì°¨ ë¶„í• : val vs test (stratify ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©)
            val_test_ratio = val_ratio / (val_ratio + test_ratio)
            temp_category_counts = temp_suff[stratify_column].value_counts()
            can_stratify_second = all(temp_category_counts >= 2)
            
            if can_stratify_second:
                val_suff, test_suff = train_test_split(
                    temp_suff,
                    test_size=(1 - val_test_ratio),
                    stratify=temp_suff[stratify_column],
                    random_state=random_state
                )
            else:
                # stratify ë¶ˆê°€ëŠ¥í•˜ë©´ ëœë¤ ë¶„í• 
                logger.warning("âš ï¸ 2ì°¨ ë¶„í• ì—ì„œ stratify ë¶ˆê°€ëŠ¥, ëœë¤ ë¶„í•  ì‚¬ìš©")
                val_suff, test_suff = train_test_split(
                    temp_suff,
                    test_size=(1 - val_test_ratio),
                    random_state=random_state
                )
            
            train_products_list.append(train_suff)
            val_products_list.append(val_suff)
            test_products_list.append(test_suff)
        except ValueError as e:
            # stratified split ì‹¤íŒ¨ ì‹œ ëœë¤ ë¶„í• ë¡œ ëŒ€ì²´
            logger.warning(f"âš ï¸ Stratified split ì‹¤íŒ¨, ëœë¤ ë¶„í• ë¡œ ëŒ€ì²´: {e}")
            train_suff, temp_suff = train_test_split(
                sufficient_products,
                test_size=(val_ratio + test_ratio),
                random_state=random_state
            )
            val_test_ratio = val_ratio / (val_ratio + test_ratio)
            val_suff, test_suff = train_test_split(
                temp_suff,
                test_size=(1 - val_test_ratio),
                random_state=random_state
            )
            train_products_list.append(train_suff)
            val_products_list.append(val_suff)
            test_products_list.append(test_suff)
    
    # 2. ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬: ëœë¤ ë¶„í•  (stratify ì—†ì´)
    if len(insufficient_products) > 0:
        # 1ì°¨ ë¶„í• : train vs (val + test)
        if len(insufficient_products) >= 2:
            train_insuff, temp_insuff = train_test_split(
                insufficient_products,
                test_size=(val_ratio + test_ratio),
                random_state=random_state
            )
            
            # 2ì°¨ ë¶„í• : val vs test
            if len(temp_insuff) >= 2:
                val_insuff, test_insuff = train_test_split(
                    temp_insuff,
                    test_size=(1 - val_test_ratio),
                    random_state=random_state
                )
            else:
                # tempê°€ 1ê°œë©´ valì— í• ë‹¹
                val_insuff = temp_insuff
                test_insuff = pd.DataFrame(columns=insufficient_products.columns)
        else:
            # 1ê°œë©´ trainì— í• ë‹¹
            train_insuff = insufficient_products
            val_insuff = pd.DataFrame(columns=insufficient_products.columns)
            test_insuff = pd.DataFrame(columns=insufficient_products.columns)
        
        train_products_list.append(train_insuff)
        val_products_list.append(val_insuff)
        test_products_list.append(test_insuff)
    
    # ê²°ê³¼ í•©ì¹˜ê¸°
    train_products = pd.concat(train_products_list, ignore_index=True) if train_products_list else pd.DataFrame()
    val_products = pd.concat(val_products_list, ignore_index=True) if val_products_list else pd.DataFrame()
    test_products = pd.concat(test_products_list, ignore_index=True) if test_products_list else pd.DataFrame()
    
    logger.info(f"ì œí’ˆ ë¶„í•  ì™„ë£Œ:")
    logger.info(f"  Train: {len(train_products):,}ê°œ ì œí’ˆ")
    logger.info(f"  Validation: {len(val_products):,}ê°œ ì œí’ˆ")
    logger.info(f"  Test: {len(test_products):,}ê°œ ì œí’ˆ")
    
    # ì œí’ˆ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë°ì´í„° ë¶„í• 
    train_product_ids = set(train_products['product_id'])
    val_product_ids = set(val_products['product_id'])
    test_product_ids = set(test_products['product_id'])
    
    train_df = df[df['product_id'].isin(train_product_ids)].copy()
    val_df = df[df['product_id'].isin(val_product_ids)].copy()
    test_df = df[df['product_id'].isin(test_product_ids)].copy()
    
    logger.info(f"ì´ë¯¸ì§€ ë¶„í•  ê²°ê³¼:")
    logger.info(f"  Train: {len(train_df):,}ê°œ ì´ë¯¸ì§€ ({len(train_df)/len(df)*100:.1f}%)")
    logger.info(f"  Validation: {len(val_df):,}ê°œ ì´ë¯¸ì§€ ({len(val_df)/len(df)*100:.1f}%)")
    logger.info(f"  Test: {len(test_df):,}ê°œ ì´ë¯¸ì§€ ({len(test_df)/len(df)*100:.1f}%)")
    
    return train_df, val_df, test_df

def verify_split_integrity(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:
    """ë¶„í•  ë¬´ê²°ì„± ê²€ì¦"""
    logger.info("=" * 60)
    logger.info("ğŸ” ë¶„í•  ë¬´ê²°ì„± ê²€ì¦")
    logger.info("=" * 60)
    
    # ì œí’ˆ ID ì¤‘ë³µ í™•ì¸
    train_products = set(train_df['product_id'])
    val_products = set(val_df['product_id'])
    test_products = set(test_df['product_id'])
    
    overlap_train_val = train_products & val_products
    overlap_train_test = train_products & test_products
    overlap_val_test = val_products & test_products
    
    if overlap_train_val or overlap_train_test or overlap_val_test:
        logger.error("âŒ ì œí’ˆ IDê°€ ì—¬ëŸ¬ ì„¸íŠ¸ì— ì¤‘ë³µë¨!")
        logger.error(f"  Train-Val ì¤‘ë³µ: {len(overlap_train_val)}ê°œ")
        logger.error(f"  Train-Test ì¤‘ë³µ: {len(overlap_train_test)}ê°œ")
        logger.error(f"  Val-Test ì¤‘ë³µ: {len(overlap_val_test)}ê°œ")
        raise ValueError("ë°ì´í„° ëˆ„ìˆ˜: ê°™ì€ ì œí’ˆì´ ì—¬ëŸ¬ ì„¸íŠ¸ì— í¬í•¨ë¨")
    else:
        logger.info("âœ… ì œí’ˆ ID ì¤‘ë³µ ì—†ìŒ - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì™„ë£Œ")
    
    # ì´ë¯¸ì§€ íƒ€ì… ë¶„í¬ í™•ì¸
    logger.info("ì´ë¯¸ì§€ íƒ€ì… ë¶„í¬ ë¹„êµ:")
    for name, df_split in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:
        type_dist = df_split['image_type'].value_counts()
        logger.info(f"  {name}: {dict(type_dist)}")
    
    # ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸
    logger.info("ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¹„êµ:")
    all_categories = set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])
    
    comparison_data = []
    for category in sorted(all_categories):
        train_count = len(train_df[train_df['_temp_category_name'] == category])
        val_count = len(val_df[val_df['_temp_category_name'] == category])
        test_count = len(test_df[test_df['_temp_category_name'] == category])
        total_count = train_count + val_count + test_count
        
        comparison_data.append({
            'Category': category,
            'Train': train_count,
            'Val': val_count,
            'Test': test_count,
            'Total': total_count,
            'Train_%': round(train_count / total_count * 100, 1) if total_count > 0 else 0,
            'Val_%': round(val_count / total_count * 100, 1) if total_count > 0 else 0,
            'Test_%': round(test_count / total_count * 100, 1) if total_count > 0 else 0
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í•  ë¶„í¬:")
    print(comparison_df.to_string(index=False))

def save_splits(train_df: pd.DataFrame, 
                val_df: pd.DataFrame, 
                test_df: pd.DataFrame,
                output_dir: str = "data") -> dict:
    """ë¶„í• ëœ ë°ì´í„° ì €ì¥ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)"""
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_paths = {
        'train': os.path.join(output_dir, 'train_data.csv'),
        'validation': os.path.join(output_dir, 'validation_data.csv'),
        'test': os.path.join(output_dir, 'test_data.csv')
    }
    
    # ìµœì¢… ì €ì¥í•  ì»¬ëŸ¼ ì„ íƒ (ì„ì‹œ ì»¬ëŸ¼ ì œê±°)
    final_columns = ['product_id', 'image_path', 'image_type']
    
    # ë°ì´í„°í”„ë ˆì„ ì €ì¥ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
    train_df[final_columns].to_csv(file_paths['train'], index=False, encoding='utf-8')
    val_df[final_columns].to_csv(file_paths['validation'], index=False, encoding='utf-8')
    test_df[final_columns].to_csv(file_paths['test'], index=False, encoding='utf-8')
    
    logger.info("=" * 60)
    logger.info("ğŸ’¾ ë¶„í• ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)")
    logger.info("=" * 60)
    logger.info(f"ì €ì¥ëœ ì»¬ëŸ¼: {', '.join(final_columns)}")
    for split_name, file_path in file_paths.items():
        logger.info(f"  {split_name.upper()}: {file_path}")
    
    return file_paths

def create_summary(train_df: pd.DataFrame, 
                  val_df: pd.DataFrame, 
                  test_df: pd.DataFrame,
                  file_paths: dict,
                  config: dict,
                  output_dir: str = "data") -> str:
    """ë¶„í•  ìš”ì•½ ì •ë³´ ìƒì„±"""
    from datetime import datetime
    
    summary = {
        'split_info': {
            'created_at': datetime.now().isoformat(),
            'source_file': config['input_file'],
            'random_state': config['random_state'],
            'train_ratio': config['train_ratio'],
            'val_ratio': config['val_ratio'],
            'test_ratio': config['test_ratio'],
            'split_method': 'product_based_stratified'
        },
        'data_counts': {
            'total_images': len(train_df) + len(val_df) + len(test_df),
            'train_images': len(train_df),
            'validation_images': len(val_df),
            'test_images': len(test_df),
            'total_products': len(set(train_df['product_id']) | set(val_df['product_id']) | set(test_df['product_id'])),
            'train_products': len(set(train_df['product_id'])),
            'validation_products': len(set(val_df['product_id'])),
            'test_products': len(set(test_df['product_id']))
        },
        'file_paths': file_paths,
        'categories': {
            'total_categories': len(set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])),
            'category_list': sorted(list(set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])))
        },
        'image_type_distribution': {
            'train': train_df['image_type'].value_counts().to_dict(),
            'validation': val_df['image_type'].value_counts().to_dict(),
            'test': test_df['image_type'].value_counts().to_dict()
        },
        'category_distribution': {
            'train': train_df['_temp_category_name'].value_counts().to_dict(),
            'validation': val_df['_temp_category_name'].value_counts().to_dict(),
            'test': test_df['_temp_category_name'].value_counts().to_dict()
        }
    }
    
    # JSONìœ¼ë¡œ ì €ì¥
    import json
    summary_path = os.path.join(output_dir, 'data_split_summary.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“‹ ë¶„í•  ìš”ì•½ ì •ë³´ ì €ì¥: {summary_path}")
    
    return summary_path

def validate_image_paths(df: pd.DataFrame, base_path: str = "") -> dict:
    """ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
    logger.info("ğŸ” ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬...")
    
    image_column = 'image_path'
    if image_column not in df.columns:
        logger.warning(f"ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {'valid_count': 0, 'invalid_count': len(df), 'missing_count': len(df)}
    
    valid_count = 0
    invalid_count = 0
    missing_count = 0
    
    for idx, row in df.iterrows():
        image_path = row[image_column]
        
        if pd.isna(image_path) or image_path == '':
            missing_count += 1
            continue
            
        full_path = os.path.join(base_path, image_path) if base_path else image_path
        
        if os.path.exists(full_path):
            valid_count += 1
        else:
            invalid_count += 1
    
    result = {
        'total_count': len(df),
        'valid_count': valid_count,
        'invalid_count': invalid_count,
        'missing_count': missing_count,
        'valid_ratio': valid_count / len(df) if len(df) > 0 else 0
    }
    
    logger.info(f"  ì´ ì´ë¯¸ì§€: {result['total_count']:,}")
    logger.info(f"  ìœ íš¨í•œ ê²½ë¡œ: {result['valid_count']:,} ({result['valid_ratio']*100:.1f}%)")
    logger.info(f"  ì˜ëª»ëœ ê²½ë¡œ: {result['invalid_count']:,}")
    logger.info(f"  ëˆ„ë½ëœ ê²½ë¡œ: {result['missing_count']:,}")
    
    return result

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--input-file', type=str, default='data/original_data/image_data.csv',
                       help='ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: image_data.csv)')
    parser.add_argument('--output-dir', type=str, default='data',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data)')
    parser.add_argument('--train-ratio', type=float, default=0.8,
                       help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1,
                       help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)')
    parser.add_argument('--test-ratio', type=float, default=0.1,
                       help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)')
    parser.add_argument('--random-state', type=int, default=42,
                       help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    parser.add_argument('--validate-images', action='store_true',
                       help='ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬ ìˆ˜í–‰')
    parser.add_argument('--image-base-path', type=str, default='',
                       help='ì´ë¯¸ì§€ ê¸°ë³¸ ê²½ë¡œ (ìœ íš¨ì„± ê²€ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ì •ë³´
    config = {
        'input_file': args.input_file,
        'output_dir': args.output_dir,
        'train_ratio': args.train_ratio,
        'val_ratio': args.val_ratio,
        'test_ratio': args.test_ratio,
        'random_state': args.random_state
    }
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“¸ ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì…ë ¥ íŒŒì¼: {args.input_file}")
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
        logger.info(f"ë¶„í•  ë¹„ìœ¨ - Train: {args.train_ratio}, Val: {args.val_ratio}, Test: {args.test_ratio}")
        logger.info(f"ëœë¤ ì‹œë“œ: {args.random_state}")
        
        # 1. ë°ì´í„° ë¡œë“œ
        if not os.path.exists(args.input_file):
            raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        
        logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {args.input_file}")
        df = pd.read_csv(args.input_file, encoding='utf-8')
        logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ì œí’ˆ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['id', 'category_name']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}")
        
        # 2. ì œí’ˆë³„ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ í™•ì¥
        expanded_df = expand_images_to_rows(df)
        
        if len(expanded_df) == 0:
            raise ValueError("í™•ì¥ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # 3. í™•ì¥ëœ ë°ì´í„° ë¶„ì„
        analysis = analyze_expanded_data(expanded_df)
        logger.info("=" * 60)
        logger.info("ğŸ“Š í™•ì¥ëœ ë°ì´í„° ë¶„ì„")
        logger.info("=" * 60)
        logger.info(f"ì´ ì´ë¯¸ì§€: {analysis['total_images']:,}ê°œ")
        logger.info(f"ì´ ì œí’ˆ: {analysis['total_products']:,}ê°œ")
        logger.info(f"ì¹´í…Œê³ ë¦¬ ìˆ˜: {analysis['num_categories']}ê°œ")
        
        logger.info("ì´ë¯¸ì§€ íƒ€ì… ë¶„í¬:")
        for img_type, count in analysis['image_type_distribution'].items():
            pct = count / analysis['total_images'] * 100
            logger.info(f"  {img_type}: {count:,}ê°œ ({pct:.1f}%)")
        
        # 4. ëˆ„ë½ê°’ ì²´í¬ ë° ì •ë¦¬
        missing_image_path = analysis['missing_values'].get('image_path', 0)
        missing_category = analysis['missing_values'].get('_temp_category_name', 0)
        
        if missing_image_path > 0:
            logger.warning(f"image_path ëˆ„ë½: {missing_image_path}ê°œ")
        if missing_category > 0:
            logger.warning(f"ì¹´í…Œê³ ë¦¬ ëˆ„ë½: {missing_category}ê°œ")
            
        # ëˆ„ë½ê°’ì´ ìˆëŠ” í–‰ ì œê±°
        original_len = len(expanded_df)
        expanded_df = expanded_df.dropna(subset=['image_path', '_temp_category_name'])
        if len(expanded_df) < original_len:
            logger.info(f"ëˆ„ë½ê°’ ì œê±°: {original_len - len(expanded_df)}ê°œ í–‰ ì œê±°ë¨")
        
        # 5. ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬ (ì„ íƒì )
        if args.validate_images:
            validate_image_paths(expanded_df, args.image_base_path)
        
        # 6. ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í• 
        train_df, val_df, test_df = stratified_split_by_product(
            expanded_df,
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio,
            random_state=args.random_state
        )
        
        # 7. ë¶„í•  ë¬´ê²°ì„± ê²€ì¦
        verify_split_integrity(train_df, val_df, test_df)
        
        # 8. ë¶„í• ëœ ë°ì´í„° ì €ì¥
        file_paths = save_splits(train_df, val_df, test_df, args.output_dir)
        
        # 9. ìš”ì•½ ì •ë³´ ìƒì„±
        summary_path = create_summary(train_df, val_df, test_df, file_paths, config, args.output_dir)
        
        logger.info("=" * 60)
        logger.info("âœ… ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        logger.info("=" * 60)
        logger.info("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        for split_name, file_path in file_paths.items():
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            logger.info(f"  {file_path} ({size_mb:.1f}MB)")
        logger.info(f"  {summary_path}")
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ‰ ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ ë°ì´í„° ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“‚ ì¶œë ¥ í´ë”: {args.output_dir}")
        print(f"ğŸ“Š ë¶„í•  ê²°ê³¼:")
        print(f"  Train: {len(train_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(train_df['product_id'])):,}ê°œ ì œí’ˆ)")
        print(f"  Val: {len(val_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(val_df['product_id'])):,}ê°œ ì œí’ˆ)")
        print(f"  Test: {len(test_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(test_df['product_id'])):,}ê°œ ì œí’ˆ)")
        
        # ì´ë¯¸ì§€ íƒ€ì… ë¶„í¬ ìš”ì•½
        print(f"ğŸ“¸ ì´ë¯¸ì§€ íƒ€ì… ë¶„í¬:")
        for img_type in sorted(train_df['image_type'].unique()):
            train_count = len(train_df[train_df['image_type'] == img_type])
            val_count = len(val_df[val_df['image_type'] == img_type])
            test_count = len(test_df[test_df['image_type'] == img_type])
            print(f"  {img_type}: Train {train_count:,}, Val {val_count:,}, Test {test_count:,}")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()