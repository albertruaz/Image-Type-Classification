#!/usr/bin/env python3
# divide_data.py
"""
ì´ë¯¸ì§€ íƒœê·¸ ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸

image_data.csv íŒŒì¼ì„ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
tag ì´ë¯¸ì§€(text_tag_image, neck_tag_image) vs ì¼ë°˜ ì´ë¯¸ì§€ë¡œ ë¶„ë¥˜í•˜ì—¬
train/validation/testë¡œ ë¶„í• í•˜ì—¬ data/ í´ë”ì— ì €ì¥

ì‚¬ìš©ë²•:
    python divide_data.py                    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¶„í• 
    python divide_data.py --train-ratio 0.7 # ì»¤ìŠ¤í…€ ë¹„ìœ¨
    python divide_data.py --random-state 123 # ì‹œë“œ ë³€ê²½
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from collections import Counter
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def expand_images_to_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì œí’ˆë³„ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ í™•ì¥
    
    Args:
        df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (ì œí’ˆë³„)
        
    Returns:
        í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ (ì´ë¯¸ì§€ë³„)
    """
    logger.info("ğŸ”„ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ ë°ì´í„° í™•ì¥ ì¤‘...")
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    expanded_rows = []
    
    # ì´ë¯¸ì§€ íƒ€ì…ë³„ ì»¬ëŸ¼ ì •ì˜
    image_type_columns = {
        'main_image': {'is_text_tag': 0, 'image_type': 'main_image'},
        'back_image': {'is_text_tag': 0, 'image_type': 'back_image'},
        'text_tag_image': {'is_text_tag': 1, 'image_type': 'text_tag_image'},
        'neck_tag_image': {'is_text_tag': 1, 'image_type': 'neck_tag_image'},
        'other': {'is_text_tag': 0, 'image_type': 'other'}
    }
    
    for idx, row in df.iterrows():
        product_id = row['id']
        
        # ê° ì´ë¯¸ì§€ íƒ€ì…ë³„ë¡œ ì²˜ë¦¬
        for column_name, label_info in image_type_columns.items():
            if column_name not in df.columns:
                continue
                
            image_paths_str = row[column_name]
            
            # ë¹ˆ ê°’ì´ë©´ ê±´ë„ˆë›°ê¸°
            if pd.isna(image_paths_str) or image_paths_str == '':
                continue
            
            # ì´ë¯¸ì§€ ê²½ë¡œë“¤ ë¶„ë¦¬ (& ë˜ëŠ” , ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            if '&' in str(image_paths_str):
                image_paths = str(image_paths_str).split('&')
            elif ',' in str(image_paths_str):
                image_paths = str(image_paths_str).split(',')
            else:
                image_paths = [str(image_paths_str)]
            
            # ê° ì´ë¯¸ì§€ ê²½ë¡œì— ëŒ€í•´ í–‰ ìƒì„±
            for image_path in image_paths:
                image_path = image_path.strip()
                if image_path == '' or image_path == 'nan':
                    continue
                
                # ìƒˆë¡œìš´ í–‰ ìƒì„± (ì´ì§„ ë¶„ë¥˜ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
                new_row = {
                    'product_id': product_id,
                    'image_path': image_path,
                    'image_type': label_info['image_type'],
                    'is_text_tag': label_info['is_text_tag'],
                    # ë¶„í•  ì‹œì—ë§Œ ì‚¬ìš©í•  ì„ì‹œ ì»¬ëŸ¼ (ìµœì¢… ì €ì¥ ì‹œ ì œê±°)
                    '_temp_category_name': row['category_name']
                }
                
                expanded_rows.append(new_row)
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    expanded_df = pd.DataFrame(expanded_rows)
    
    logger.info(f"í™•ì¥ ì™„ë£Œ:")
    logger.info(f"  ì›ë³¸: {len(df)}ê°œ ì œí’ˆ")
    logger.info(f"  í™•ì¥í›„: {len(expanded_df)}ê°œ ì´ë¯¸ì§€")
    
    # íƒœê·¸ë³„ ë¶„í¬ í™•ì¸
    tag_distribution = expanded_df['is_text_tag'].value_counts()
    logger.info(f"íƒœê·¸ ë¶„í¬:")
    logger.info(f"  ì¼ë°˜ ì´ë¯¸ì§€ (0): {tag_distribution.get(0, 0):,}ê°œ")
    logger.info(f"  íƒœê·¸ ì´ë¯¸ì§€ (1): {tag_distribution.get(1, 0):,}ê°œ")
    
    # ì´ë¯¸ì§€ íƒ€ì…ë³„ ë¶„í¬
    type_distribution = expanded_df['image_type'].value_counts()
    logger.info(f"ì´ë¯¸ì§€ íƒ€ì…ë³„ ë¶„í¬:")
    for img_type, count in type_distribution.items():
        logger.info(f"  {img_type}: {count:,}ê°œ")
    
    return expanded_df

def analyze_expanded_data(df: pd.DataFrame) -> dict:
    """í™•ì¥ëœ ë°ì´í„° ë¶„ì„"""
    analysis = {
        'total_images': len(df),
        'total_products': df['product_id'].nunique(),
        'columns': list(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'tag_distribution': df['is_text_tag'].value_counts().to_dict(),
        'image_type_distribution': df['image_type'].value_counts().to_dict(),
        'category_distribution': df['_temp_category_name'].value_counts().to_dict(),
        'categories': df['_temp_category_name'].unique().tolist(),
        'num_categories': df['_temp_category_name'].nunique()
    }
    
    return analysis

def stratified_split_by_product(df: pd.DataFrame, 
                               train_ratio: float = 0.8,
                               val_ratio: float = 0.1,
                               test_ratio: float = 0.1,
                               random_state: int = 42) -> tuple:
    """
    ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í•  (ê°™ì€ ì œí’ˆì˜ ì´ë¯¸ì§€ë“¤ì´ ë‹¤ë¥¸ ì„¸íŠ¸ì— ì„ì´ì§€ ì•Šë„ë¡)
    
    Args:
        df: í™•ì¥ëœ ë°ì´í„°í”„ë ˆì„ (ì´ë¯¸ì§€ë³„)
        train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
        val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        random_state: ëœë¤ ì‹œë“œ
        
    Returns:
        train_df, val_df, test_df
    """
    # ë¹„ìœ¨ ê²€ì¦
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio}")
    
    logger.info("ğŸ”„ ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í•  ì¤‘...")
    
    # ì œí’ˆë³„ ì§‘ê³„ (ì¹´í…Œê³ ë¦¬ì™€ íƒœê·¸ ë¶„í¬ ê³„ì‚°)
    product_summary = df.groupby('product_id').agg({
        '_temp_category_name': 'first',  # ì œí’ˆì˜ ì¹´í…Œê³ ë¦¬ (ì„ì‹œ)
        'is_text_tag': ['sum', 'count']  # íƒœê·¸ ì´ë¯¸ì§€ ìˆ˜, ì „ì²´ ì´ë¯¸ì§€ ìˆ˜
    }).reset_index()
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    product_summary.columns = ['product_id', '_temp_category_name', 'tag_count', 'total_count']
    product_summary['has_tag'] = (product_summary['tag_count'] > 0).astype(int)
    
    logger.info(f"ì œí’ˆ ë¶„í¬:")
    logger.info(f"  ì „ì²´ ì œí’ˆ: {len(product_summary):,}ê°œ")
    logger.info(f"  íƒœê·¸ ì´ë¯¸ì§€ ìˆëŠ” ì œí’ˆ: {product_summary['has_tag'].sum():,}ê°œ")
    logger.info(f"  íƒœê·¸ ì´ë¯¸ì§€ ì—†ëŠ” ì œí’ˆ: {(product_summary['has_tag'] == 0).sum():,}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
    category_dist = product_summary['_temp_category_name'].value_counts()
    logger.info(f"ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for category, count in category_dist.items():
        logger.info(f"  {category}: {count:,}ê°œ")
    
    # 1ì°¨ ë¶„í• : train vs (val + test)
    train_products, temp_products = train_test_split(
        product_summary,
        test_size=(val_ratio + test_ratio),
        stratify=product_summary['_temp_category_name'],
        random_state=random_state
    )
    
    # 2ì°¨ ë¶„í• : val vs test
    val_test_ratio = val_ratio / (val_ratio + test_ratio)
    val_products, test_products = train_test_split(
        temp_products,
        test_size=(1 - val_test_ratio),
        stratify=temp_products['_temp_category_name'],
        random_state=random_state
    )
    
    logger.info(f"ì œí’ˆ ë¶„í•  ì™„ë£Œ:")
    logger.info(f"  Train: {len(train_products):,}ê°œ ì œí’ˆ")
    logger.info(f"  Validation: {len(val_products):,}ê°œ ì œí’ˆ")
    logger.info(f"  Test: {len(test_products):,}ê°œ ì œí’ˆ")
    
    # ì œí’ˆ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë°ì´í„° ë¶„í• 
    train_product_ids = set(train_products['product_id'])
    val_product_ids = set(val_products['product_id'])
    test_product_ids = set(test_products['product_id'])
    
    train_df = df[df['product_id'].isin(train_product_ids)].copy()
    val_df = df[df['product_id'].isin(val_product_ids)].copy()
    test_df = df[df['product_id'].isin(test_product_ids)].copy()
    
    logger.info(f"ì´ë¯¸ì§€ ë¶„í•  ê²°ê³¼:")
    logger.info(f"  Train: {len(train_df):,}ê°œ ì´ë¯¸ì§€ ({len(train_df)/len(df)*100:.1f}%)")
    logger.info(f"  Validation: {len(val_df):,}ê°œ ì´ë¯¸ì§€ ({len(val_df)/len(df)*100:.1f}%)")
    logger.info(f"  Test: {len(test_df):,}ê°œ ì´ë¯¸ì§€ ({len(test_df)/len(df)*100:.1f}%)")
    
    return train_df, val_df, test_df

def verify_split_integrity(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:
    """ë¶„í•  ë¬´ê²°ì„± ê²€ì¦"""
    logger.info("=" * 60)
    logger.info("ğŸ” ë¶„í•  ë¬´ê²°ì„± ê²€ì¦")
    logger.info("=" * 60)
    
    # ì œí’ˆ ID ì¤‘ë³µ í™•ì¸
    train_products = set(train_df['product_id'])
    val_products = set(val_df['product_id'])
    test_products = set(test_df['product_id'])
    
    overlap_train_val = train_products & val_products
    overlap_train_test = train_products & test_products
    overlap_val_test = val_products & test_products
    
    if overlap_train_val or overlap_train_test or overlap_val_test:
        logger.error("âŒ ì œí’ˆ IDê°€ ì—¬ëŸ¬ ì„¸íŠ¸ì— ì¤‘ë³µë¨!")
        logger.error(f"  Train-Val ì¤‘ë³µ: {len(overlap_train_val)}ê°œ")
        logger.error(f"  Train-Test ì¤‘ë³µ: {len(overlap_train_test)}ê°œ")
        logger.error(f"  Val-Test ì¤‘ë³µ: {len(overlap_val_test)}ê°œ")
        raise ValueError("ë°ì´í„° ëˆ„ìˆ˜: ê°™ì€ ì œí’ˆì´ ì—¬ëŸ¬ ì„¸íŠ¸ì— í¬í•¨ë¨")
    else:
        logger.info("âœ… ì œí’ˆ ID ì¤‘ë³µ ì—†ìŒ - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ì™„ë£Œ")
    
    # íƒœê·¸ ë¶„í¬ í™•ì¸
    logger.info("íƒœê·¸ ë¶„í¬ ë¹„êµ:")
    for name, df_split in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:
        tag_dist = df_split['is_text_tag'].value_counts()
        total = len(df_split)
        tag_ratio = tag_dist.get(1, 0) / total * 100 if total > 0 else 0
        logger.info(f"  {name}: íƒœê·¸ {tag_dist.get(1, 0):,}ê°œ / ì „ì²´ {total:,}ê°œ ({tag_ratio:.1f}%)")
    
    # ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸
    logger.info("ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¹„êµ:")
    all_categories = set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])
    
    comparison_data = []
    for category in sorted(all_categories):
        train_count = len(train_df[train_df['_temp_category_name'] == category])
        val_count = len(val_df[val_df['_temp_category_name'] == category])
        test_count = len(test_df[test_df['_temp_category_name'] == category])
        total_count = train_count + val_count + test_count
        
        comparison_data.append({
            'Category': category,
            'Train': train_count,
            'Val': val_count,
            'Test': test_count,
            'Total': total_count,
            'Train_%': round(train_count / total_count * 100, 1) if total_count > 0 else 0,
            'Val_%': round(val_count / total_count * 100, 1) if total_count > 0 else 0,
            'Test_%': round(test_count / total_count * 100, 1) if total_count > 0 else 0
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í•  ë¶„í¬:")
    print(comparison_df.to_string(index=False))

def save_splits(train_df: pd.DataFrame, 
                val_df: pd.DataFrame, 
                test_df: pd.DataFrame,
                output_dir: str = "data") -> dict:
    """ë¶„í• ëœ ë°ì´í„° ì €ì¥ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)"""
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_paths = {
        'train': os.path.join(output_dir, 'train_data.csv'),
        'validation': os.path.join(output_dir, 'validation_data.csv'),
        'test': os.path.join(output_dir, 'test_data.csv')
    }
    
    # ìµœì¢… ì €ì¥í•  ì»¬ëŸ¼ ì„ íƒ (ì„ì‹œ ì»¬ëŸ¼ ì œê±°)
    final_columns = ['product_id', 'image_path', 'image_type', 'is_text_tag']
    
    # ë°ì´í„°í”„ë ˆì„ ì €ì¥ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
    train_df[final_columns].to_csv(file_paths['train'], index=False, encoding='utf-8')
    val_df[final_columns].to_csv(file_paths['validation'], index=False, encoding='utf-8')
    test_df[final_columns].to_csv(file_paths['test'], index=False, encoding='utf-8')
    
    logger.info("=" * 60)
    logger.info("ğŸ’¾ ë¶„í• ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)")
    logger.info("=" * 60)
    logger.info(f"ì €ì¥ëœ ì»¬ëŸ¼: {', '.join(final_columns)}")
    for split_name, file_path in file_paths.items():
        logger.info(f"  {split_name.upper()}: {file_path}")
    
    return file_paths

def create_summary(train_df: pd.DataFrame, 
                  val_df: pd.DataFrame, 
                  test_df: pd.DataFrame,
                  file_paths: dict,
                  config: dict,
                  output_dir: str = "data") -> str:
    """ë¶„í•  ìš”ì•½ ì •ë³´ ìƒì„±"""
    from datetime import datetime
    
    summary = {
        'split_info': {
            'created_at': datetime.now().isoformat(),
            'source_file': config['input_file'],
            'random_state': config['random_state'],
            'train_ratio': config['train_ratio'],
            'val_ratio': config['val_ratio'],
            'test_ratio': config['test_ratio'],
            'split_method': 'product_based_stratified'
        },
        'data_counts': {
            'total_images': len(train_df) + len(val_df) + len(test_df),
            'train_images': len(train_df),
            'validation_images': len(val_df),
            'test_images': len(test_df),
            'total_products': len(set(train_df['product_id']) | set(val_df['product_id']) | set(test_df['product_id'])),
            'train_products': len(set(train_df['product_id'])),
            'validation_products': len(set(val_df['product_id'])),
            'test_products': len(set(test_df['product_id']))
        },
        'file_paths': file_paths,
        'categories': {
            'total_categories': len(set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])),
            'category_list': sorted(list(set(train_df['_temp_category_name']) | set(val_df['_temp_category_name']) | set(test_df['_temp_category_name'])))
        },
        'tag_distribution': {
            'train': train_df['is_text_tag'].value_counts().to_dict(),
            'validation': val_df['is_text_tag'].value_counts().to_dict(),
            'test': test_df['is_text_tag'].value_counts().to_dict()
        },
        'image_type_distribution': {
            'train': train_df['image_type'].value_counts().to_dict(),
            'validation': val_df['image_type'].value_counts().to_dict(),
            'test': test_df['image_type'].value_counts().to_dict()
        },
        'category_distribution': {
            'train': train_df['_temp_category_name'].value_counts().to_dict(),
            'validation': val_df['_temp_category_name'].value_counts().to_dict(),
            'test': test_df['_temp_category_name'].value_counts().to_dict()
        }
    }
    
    # JSONìœ¼ë¡œ ì €ì¥
    import json
    summary_path = os.path.join(output_dir, 'data_split_summary.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“‹ ë¶„í•  ìš”ì•½ ì •ë³´ ì €ì¥: {summary_path}")
    
    return summary_path

def validate_image_paths(df: pd.DataFrame, base_path: str = "") -> dict:
    """ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
    logger.info("ğŸ” ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬...")
    
    image_column = 'image_path'
    if image_column not in df.columns:
        logger.warning(f"ì´ë¯¸ì§€ ì»¬ëŸ¼ '{image_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {'valid_count': 0, 'invalid_count': len(df), 'missing_count': len(df)}
    
    valid_count = 0
    invalid_count = 0
    missing_count = 0
    
    for idx, row in df.iterrows():
        image_path = row[image_column]
        
        if pd.isna(image_path) or image_path == '':
            missing_count += 1
            continue
            
        full_path = os.path.join(base_path, image_path) if base_path else image_path
        
        if os.path.exists(full_path):
            valid_count += 1
        else:
            invalid_count += 1
    
    result = {
        'total_count': len(df),
        'valid_count': valid_count,
        'invalid_count': invalid_count,
        'missing_count': missing_count,
        'valid_ratio': valid_count / len(df) if len(df) > 0 else 0
    }
    
    logger.info(f"  ì´ ì´ë¯¸ì§€: {result['total_count']:,}")
    logger.info(f"  ìœ íš¨í•œ ê²½ë¡œ: {result['valid_count']:,} ({result['valid_ratio']*100:.1f}%)")
    logger.info(f"  ì˜ëª»ëœ ê²½ë¡œ: {result['invalid_count']:,}")
    logger.info(f"  ëˆ„ë½ëœ ê²½ë¡œ: {result['missing_count']:,}")
    
    return result

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì´ë¯¸ì§€ íƒœê·¸ ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--input-file', type=str, default='data/original_data/image_data.csv',
                       help='ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: image_data.csv)')
    parser.add_argument('--output-dir', type=str, default='data',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data)')
    parser.add_argument('--train-ratio', type=float, default=0.8,
                       help='í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1,
                       help='ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)')
    parser.add_argument('--test-ratio', type=float, default=0.1,
                       help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)')
    parser.add_argument('--random-state', type=int, default=42,
                       help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    parser.add_argument('--validate-images', action='store_true',
                       help='ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬ ìˆ˜í–‰')
    parser.add_argument('--image-base-path', type=str, default='',
                       help='ì´ë¯¸ì§€ ê¸°ë³¸ ê²½ë¡œ (ìœ íš¨ì„± ê²€ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ì •ë³´
    config = {
        'input_file': args.input_file,
        'output_dir': args.output_dir,
        'train_ratio': args.train_ratio,
        'val_ratio': args.val_ratio,
        'test_ratio': args.test_ratio,
        'random_state': args.random_state
    }
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ·ï¸ ì´ë¯¸ì§€ íƒœê·¸ ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"ì…ë ¥ íŒŒì¼: {args.input_file}")
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
        logger.info(f"ë¶„í•  ë¹„ìœ¨ - Train: {args.train_ratio}, Val: {args.val_ratio}, Test: {args.test_ratio}")
        logger.info(f"ëœë¤ ì‹œë“œ: {args.random_state}")
        
        # 1. ë°ì´í„° ë¡œë“œ
        if not os.path.exists(args.input_file):
            raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        
        logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {args.input_file}")
        df = pd.read_csv(args.input_file, encoding='utf-8')
        logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ì œí’ˆ")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['id', 'category_name']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}")
        
        # 2. ì œí’ˆë³„ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë³„ í–‰ìœ¼ë¡œ í™•ì¥
        expanded_df = expand_images_to_rows(df)
        
        if len(expanded_df) == 0:
            raise ValueError("í™•ì¥ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # 3. í™•ì¥ëœ ë°ì´í„° ë¶„ì„
        analysis = analyze_expanded_data(expanded_df)
        logger.info("=" * 60)
        logger.info("ğŸ“Š í™•ì¥ëœ ë°ì´í„° ë¶„ì„")
        logger.info("=" * 60)
        logger.info(f"ì´ ì´ë¯¸ì§€: {analysis['total_images']:,}ê°œ")
        logger.info(f"ì´ ì œí’ˆ: {analysis['total_products']:,}ê°œ")
        logger.info(f"ì¹´í…Œê³ ë¦¬ ìˆ˜: {analysis['num_categories']}ê°œ")
        
        logger.info("íƒœê·¸ ë¶„í¬:")
        for tag_value, count in analysis['tag_distribution'].items():
            tag_name = "íƒœê·¸ ì´ë¯¸ì§€" if tag_value == 1 else "ì¼ë°˜ ì´ë¯¸ì§€"
            pct = count / analysis['total_images'] * 100
            logger.info(f"  {tag_name} ({tag_value}): {count:,}ê°œ ({pct:.1f}%)")
        
        # 4. ëˆ„ë½ê°’ ì²´í¬ ë° ì •ë¦¬
        missing_image_path = analysis['missing_values'].get('image_path', 0)
        missing_category = analysis['missing_values'].get('_temp_category_name', 0)
        
        if missing_image_path > 0:
            logger.warning(f"image_path ëˆ„ë½: {missing_image_path}ê°œ")
        if missing_category > 0:
            logger.warning(f"ì¹´í…Œê³ ë¦¬ ëˆ„ë½: {missing_category}ê°œ")
            
        # ëˆ„ë½ê°’ì´ ìˆëŠ” í–‰ ì œê±°
        original_len = len(expanded_df)
        expanded_df = expanded_df.dropna(subset=['image_path', '_temp_category_name'])
        if len(expanded_df) < original_len:
            logger.info(f"ëˆ„ë½ê°’ ì œê±°: {original_len - len(expanded_df)}ê°œ í–‰ ì œê±°ë¨")
        
        # 5. ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬ (ì„ íƒì )
        if args.validate_images:
            validate_image_paths(expanded_df, args.image_base_path)
        
        # 6. ì œí’ˆ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í• 
        train_df, val_df, test_df = stratified_split_by_product(
            expanded_df,
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            test_ratio=args.test_ratio,
            random_state=args.random_state
        )
        
        # 7. ë¶„í•  ë¬´ê²°ì„± ê²€ì¦
        verify_split_integrity(train_df, val_df, test_df)
        
        # 8. ë¶„í• ëœ ë°ì´í„° ì €ì¥
        file_paths = save_splits(train_df, val_df, test_df, args.output_dir)
        
        # 9. ìš”ì•½ ì •ë³´ ìƒì„±
        summary_path = create_summary(train_df, val_df, test_df, file_paths, config, args.output_dir)
        
        logger.info("=" * 60)
        logger.info("âœ… ì´ë¯¸ì§€ íƒœê·¸ ë¶„ë¥˜ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        logger.info("=" * 60)
        logger.info("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        for split_name, file_path in file_paths.items():
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            logger.info(f"  {file_path} ({size_mb:.1f}MB)")
        logger.info(f"  {summary_path}")
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ‰ ì´ë¯¸ì§€ íƒœê·¸ ë¶„ë¥˜ ë°ì´í„° ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“‚ ì¶œë ¥ í´ë”: {args.output_dir}")
        print(f"ğŸ“Š ë¶„í•  ê²°ê³¼:")
        print(f"  Train: {len(train_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(train_df['product_id'])):,}ê°œ ì œí’ˆ)")
        print(f"  Val: {len(val_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(val_df['product_id'])):,}ê°œ ì œí’ˆ)")
        print(f"  Test: {len(test_df):,}ê°œ ì´ë¯¸ì§€ ({len(set(test_df['product_id'])):,}ê°œ ì œí’ˆ)")
        
        # íƒœê·¸ ë¶„í¬ ìš”ì•½
        train_tag_ratio = train_df['is_text_tag'].mean() * 100
        val_tag_ratio = val_df['is_text_tag'].mean() * 100
        test_tag_ratio = test_df['is_text_tag'].mean() * 100
        print(f"ğŸ·ï¸ íƒœê·¸ ì´ë¯¸ì§€ ë¹„ìœ¨:")
        print(f"  Train: {train_tag_ratio:.1f}%, Val: {val_tag_ratio:.1f}%, Test: {test_tag_ratio:.1f}%")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()