#!/usr/bin/env python3
# tune_hyperparameters.py
"""
ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

Validation ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ê³ 
ìµœì ì˜ ì„¤ì •ì„ ì°¾ìŠµë‹ˆë‹¤. main.pyì˜ ImageTypeClassificationPipelineì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python tune_hyperparameters.py                      # ê¶Œì¥ ì„¤ì • í…ŒìŠ¤íŠ¸
    python tune_hyperparameters.py --quick-test         # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3 ì—í­)
    python tune_hyperparameters.py --mode grid          # ê·¸ë¦¬ë“œ ì„œì¹˜
"""

import argparse
import json
import os
import sys
import logging
import copy
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import itertools
import random

import torch
import pandas as pd

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# main.pyì—ì„œ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
from main import ImageTypeClassificationPipeline
from utils.device_manager import DeviceManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
# ============================================================================

# ìµœì¢… ê¸°ì¤€ ì„¸íŒ… + ì¶”ê°€ë¡œ ë§Œì§ˆ ë§Œí•œ í¬ì¸íŠ¸ë§Œ êµ¬ì„±:
# - ê¸°ì¤€: focal_gamma=1.0, class_weight ë¯¸ì‚¬ìš©, dropout 0.2, cosine + warmup 1
# - ì‹¤í—˜: class-wise threshold, full_shotë§Œ ë‚®ì¶˜ per-class gamma

RECOMMENDED_CONFIGS = [
    {
        'name': 'baseline_fixed',
        'description': 'ìµœì¢… ê¸°ì¤€ ì„¸íŒ… ì¬í˜„',
        'overrides': {
            'training.focal_loss': True,
            'training.focal_gamma': 1.0,
            'training.focal_alpha': 0.25,
            'training.use_class_weights': False,
            'training.learning_rate': 1e-4,
            'training.scheduler': 'cosine',
            'training.warmup_epochs': 1,
            'augmentation.strength': 'medium',
            'model.dropout_rate': 0.2,
        }
    },
    {
        'name': 'classwise_thresholds',
        'description': 'í´ë˜ìŠ¤ë³„ threshold í›„ì²˜ë¦¬ë¡œ recall ë³´ì •',
        'overrides': {
            'training.focal_loss': True,
            'training.focal_gamma': 1.0,
            'training.focal_alpha': 0.25,
            'training.use_class_weights': False,
            'training.learning_rate': 1e-4,
            'training.scheduler': 'cosine',
            'training.warmup_epochs': 1,
            'augmentation.strength': 'medium',
            'model.dropout_rate': 0.2,
            'inference.class_thresholds': {
                'care_label': 0.50,
                'detail_shot': 0.55,
                'full_shot': 0.35,
                'neck_label': 0.50
            }
        }
    },
    {
        'name': 'focal_gamma_full_shot_low',
        'description': 'full_shotë§Œ ì•½í•œ gammaë¡œ recall íšŒë³µ',
        'overrides': {
            'training.focal_loss': True,
            'training.focal_alpha': 0.25,
            'training.use_class_weights': False,
            'training.learning_rate': 1e-4,
            'training.scheduler': 'cosine',
            'training.warmup_epochs': 1,
            'augmentation.strength': 'medium',
            'model.dropout_rate': 0.2,
            'training.focal_gamma': {
                'care_label': 1.0,
                'detail_shot': 1.0,
                'full_shot': 0.5,
                'neck_label': 1.0
            }
        }
    }
]


def apply_overrides(config: Dict, overrides: Dict[str, Any]) -> Dict:
    """
    ì„¤ì •ì— ì˜¤ë²„ë¼ì´ë“œ ì ìš©
    
    Args:
        config: ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        overrides: 'section.key': value í˜•íƒœì˜ ì˜¤ë²„ë¼ì´ë“œ
        
    Returns:
        ìˆ˜ì •ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    config = copy.deepcopy(config)
    
    for key_path, value in overrides.items():
        parts = key_path.split('.')
        target = config
        for part in parts[:-1]:
            if part not in target:
                target[part] = {}
            target = target[part]
        target[parts[-1]] = value
    
    return config


def run_single_experiment(
    base_config_path: str,
    experiment_name: str,
    overrides: Dict[str, Any],
    output_dir: str,
    epochs: int,
    use_wandb: bool = False
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ íŠœë‹ ì‹¤í—˜ ì‹¤í–‰ (main.pyì˜ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©)
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ§ª ì‹¤í—˜ ì‹œì‘: {experiment_name}")
    logger.info(f"   ì˜¤ë²„ë¼ì´ë“œ: {overrides}")
    logger.info(f"{'='*60}")
    
    wandb_run = None
    try:
        # íŒŒì´í”„ë¼ì¸ ìƒì„± (ê¸°ë³¸ ì„¤ì • ë¡œë“œ)
        pipeline = ImageTypeClassificationPipeline(config_path=base_config_path)
        
        # ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        pipeline.config = apply_overrides(pipeline.config, overrides)
        
        # íŠœë‹ìš© ì„¤ì • ì¡°ì •
        pipeline.config['training']['epochs'] = epochs
        pipeline.config['training']['patience'] = max(epochs // 2, 5)
        pipeline.config['logging']['use_wandb'] = use_wandb
        pipeline.config['logging']['wandb_prefix'] = 'tuning_'
        
        # ì‹¤í—˜ëª… ê¸°ë°˜ìœ¼ë¡œ run_id ìˆ˜ì •
        pipeline.run_id = f"tune_{experiment_name}_{datetime.now().strftime('%H%M%S')}"
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìˆ˜ì •
        experiment_dir = os.path.join(output_dir, experiment_name)
        pipeline.run_paths = {
            'run_dir': experiment_dir,
            'result_dir': experiment_dir,
            'model_dir': os.path.join(experiment_dir, 'model'),
            'log_dir': os.path.join(experiment_dir, 'logs'),
            'checkpoint_dir': os.path.join(experiment_dir, 'checkpoints')
        }
        for path in pipeline.run_paths.values():
            os.makedirs(path, exist_ok=True)
        
        # configì˜ pathsë„ ì—…ë°ì´íŠ¸ (trainerê°€ configì—ì„œ ê²½ë¡œë¥¼ ì½ìŒ)
        pipeline.config['paths']['result_dir'] = experiment_dir
        pipeline.config['paths']['model_dir'] = pipeline.run_paths['model_dir']
        pipeline.config['paths']['log_dir'] = pipeline.run_paths['log_dir']
        pipeline.config['paths']['checkpoint_dir'] = pipeline.run_paths['checkpoint_dir']
        
        # ì„¤ì • ì €ì¥
        config_save_path = os.path.join(experiment_dir, 'config.json')
        with open(config_save_path, 'w', encoding='utf-8') as f:
            json.dump(pipeline.config, f, indent=2, ensure_ascii=False)

        # wandb ì´ˆê¸°í™” (íŠœë‹ ì‹¤í—˜ ë‹¨ìœ„)
        if use_wandb and WANDB_AVAILABLE:
            try:
                wandb_run = wandb.init(
                    project=pipeline.config.get('logging', {}).get('wandb_project', 'image-classification'),
                    entity=pipeline.config.get('logging', {}).get('wandb_entity', None),
                    config=pipeline.config,
                    name=f"tuning_{pipeline.run_id}",
                    tags=['image-classification', 'pytorch', 'tuning', experiment_name]
                )
                logger.info(f"wandb ì‹¤í–‰ ì‹œì‘: {wandb.run.url}")
            except Exception as e:
                logger.warning(f"wandb ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                wandb_run = None

        # í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (main.pyì˜ ë©”ì„œë“œ ì¬ì‚¬ìš©)
        # ë°ì´í„° ë¡œë“œ
        train_df, val_df, test_df = pipeline.load_data()
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader, val_loader, test_loader, label_encoder = pipeline.create_data_loaders(
            train_df, val_df, test_df
        )
        
        # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        model = pipeline.create_model(pipeline.NUM_CLASSES)
        training_results = pipeline.train_model(model, train_loader, val_loader)
        
        # Validationìœ¼ë¡œ í‰ê°€ (testëŠ” ìµœì¢… í‰ê°€ìš©ìœ¼ë¡œ ë³´ì¡´)
        from image_classification.evaluator import ModelEvaluator
        evaluator = ModelEvaluator(
            model=model,
            test_loader=val_loader,  # validation ì‚¬ìš©
            class_names=pipeline.CLASS_NAMES,
            device=pipeline.device,
            save_dir=experiment_dir,
            class_thresholds=pipeline.config.get('inference', {}).get('class_thresholds')
        )
        val_eval_results = evaluator.evaluate(save_results=False)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'experiment_name': experiment_name,
            'overrides': overrides,
            'status': 'success',
            'best_val_loss': training_results.get('best_val_loss', float('inf')),
            'best_val_accuracy': training_results.get('best_val_accuracy', 0),
            'val_f1_weighted': val_eval_results['metrics'].get('f1_weighted', 0),
            'val_f1_macro': val_eval_results['metrics'].get('f1_macro', 0),
            'val_accuracy': val_eval_results['metrics'].get('accuracy', 0),
            'training_time': training_results.get('training_time', 0),
        }
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        for cls in pipeline.CLASS_NAMES:
            cls_report = val_eval_results['classification_report'].get(cls, {})
            results[f'{cls}_precision'] = cls_report.get('precision', 0)
            results[f'{cls}_recall'] = cls_report.get('recall', 0)
            results[f'{cls}_f1'] = cls_report.get('f1-score', 0)

        if wandb_run:
            try:
                prefix = pipeline.config.get('logging', {}).get('wandb_prefix', '')
                summary_metrics = {
                    'final/experiment_name': experiment_name,
                    'final/val_accuracy': results['val_accuracy'],
                    'final/val_f1_weighted': results['val_f1_weighted'],
                    'final/val_f1_macro': results['val_f1_macro'],
                    'final/best_val_loss': results['best_val_loss'],
                    'final/best_val_accuracy': results['best_val_accuracy']
                }
                if prefix:
                    summary_metrics = {f"{prefix}{key}": value for key, value in summary_metrics.items()}
                wandb.log(summary_metrics)
            except Exception as e:
                logger.warning(f"wandb ìš”ì•½ ë¡œê¹… ì‹¤íŒ¨: {e}")

        # ê²°ê³¼ ì €ì¥
        results_path = os.path.join(experiment_dir, 'tuning_result.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {experiment_name}")
        logger.info(f"   Val Accuracy: {results['val_accuracy']:.4f}")
        logger.info(f"   Val F1 (weighted): {results['val_f1_weighted']:.4f}")
        logger.info(f"   detail_shot F1: {results.get('detail_shot_f1', 0):.4f}")
        logger.info(f"   full_shot F1: {results.get('full_shot_f1', 0):.4f}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, pipeline, evaluator
        torch.cuda.empty_cache()
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {experiment_name} - {e}")
        import traceback
        traceback.print_exc()
        return {
            'experiment_name': experiment_name,
            'overrides': overrides,
            'status': 'failed',
            'error': str(e)
        }
    finally:
        if wandb_run:
            try:
                wandb.finish()
                logger.info("wandb ì‹¤í–‰ ì¢…ë£Œ")
            except Exception as e:
                logger.warning(f"wandb ì¢…ë£Œ ì‹¤íŒ¨: {e}")


def run_tuning(
    base_config_path: str,
    output_dir: str,
    configs: List[Dict],
    epochs: int,
    use_wandb: bool = False
) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ì„¤ì •ìœ¼ë¡œ íŠœë‹ ì‹¤í–‰
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(output_dir, f"tuning_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info(f"ğŸ“ íŠœë‹ ê²°ê³¼ ë””ë ‰í† ë¦¬: {output_dir}")
    logger.info(f"ğŸ“Š ì´ {len(configs)}ê°œ ì‹¤í—˜ ì˜ˆì •")
    
    all_results = []
    
    for i, cfg in enumerate(configs):
        logger.info(f"\n[{i+1}/{len(configs)}] {cfg['name']}: {cfg.get('description', '')}")
        
        results = run_single_experiment(
            base_config_path=base_config_path,
            experiment_name=cfg['name'],
            overrides=cfg['overrides'],
            output_dir=output_dir,
            epochs=epochs,
            use_wandb=use_wandb
        )
        
        results['description'] = cfg.get('description', '')
        all_results.append(results)
    
    # ê²°ê³¼ DataFrame ìƒì„±
    results_df = pd.DataFrame(all_results)
    
    # ì„±ê³µí•œ ì‹¤í—˜ë§Œ ì •ë ¬
    success_df = results_df[results_df['status'] == 'success']
    if len(success_df) > 0:
        success_df = success_df.sort_values('val_f1_weighted', ascending=False)
    
    # ê²°ê³¼ ì €ì¥
    results_df.to_csv(os.path.join(output_dir, 'tuning_results.csv'), index=False)
    
    with open(os.path.join(output_dir, 'tuning_results.json'), 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # ìµœì¢… ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š íŠœë‹ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 60)
    
    if len(success_df) > 0:
        print("\nğŸ† Validation F1 (weighted) ê¸°ì¤€ ìƒìœ„ ê²°ê³¼:")
        display_cols = ['experiment_name', 'val_f1_weighted', 'val_accuracy', 
                        'detail_shot_f1', 'full_shot_f1']
        available_cols = [c for c in display_cols if c in success_df.columns]
        print(success_df[available_cols].to_string(index=False))
        
        # ìµœì  ì„¤ì • ì €ì¥
        best_name = success_df.iloc[0]['experiment_name']
        best_overrides = success_df.iloc[0].get('overrides', {})
        
        # ìµœì  config ìƒì„±
        from utils.config_manager import ConfigManager
        config_manager = ConfigManager(base_config_path)
        best_config = apply_overrides(config_manager.get_config(), best_overrides)
        
        best_config_path = os.path.join(output_dir, 'best_config.json')
        with open(best_config_path, 'w', encoding='utf-8') as f:
            json.dump(best_config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nâœ… ìµœì  ì„¤ì • ì €ì¥: {best_config_path}")
        logger.info(f"   ìµœì  ì‹¤í—˜: {best_name}")
        logger.info(f"   Val F1 (weighted): {success_df.iloc[0]['val_f1_weighted']:.4f}")
    
    return results_df


def generate_grid_configs(max_experiments: int = 20) -> List[Dict]:
    """ê·¸ë¦¬ë“œ ì„œì¹˜ìš© ì„¤ì • ìƒì„±"""
    grid_params = {
        'training.learning_rate': [5e-5, 1e-4, 2e-4],
        'model.dropout_rate': [0.3, 0.4],
        'training.focal_loss': [True],
        'training.focal_gamma': [2.0, 3.0],
    }
    
    keys = list(grid_params.keys())
    combinations = list(itertools.product(*[grid_params[k] for k in keys]))
    
    if len(combinations) > max_experiments:
        combinations = random.sample(combinations, max_experiments)
    
    configs = []
    for i, combo in enumerate(combinations):
        overrides = dict(zip(keys, combo))
        configs.append({
            'name': f'grid_{i+1:03d}',
            'description': f'Grid search #{i+1}',
            'overrides': overrides
        })
    
    return configs


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì´ë¯¸ì§€ íƒ€ì… ë¶„ë¥˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹')
    parser.add_argument('--config', type=str, default='config.json',
                        help='ê¸°ë³¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', type=str, default='tuning_results',
                        help='íŠœë‹ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--mode', type=str, default='recommended',
                        choices=['recommended', 'grid'],
                        help='íŠœë‹ ëª¨ë“œ')
    parser.add_argument('--epochs', type=int, default=15,
                        help='ê° ì‹¤í—˜ë‹¹ ì—í­ ìˆ˜')
    parser.add_argument('--quick-test', action='store_true',
                        help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (3 ì—í­)')
    parser.add_argument('--max-experiments', type=int, default=20,
                        help='ìµœëŒ€ ì‹¤í—˜ ìˆ˜ (grid ëª¨ë“œ)')
    parser.add_argument('--use-wandb', action='store_true',
                        help='wandb ë¡œê¹… ì‚¬ìš©')
    parser.add_argument('--no-wandb', action='store_true',
                        help='wandb ë¡œê¹… ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    if args.use_wandb and args.no_wandb:
        logger.error("--use-wandb ì™€ --no-wandb ëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì—í­ ì„¤ì •
    epochs = 3 if args.quick_test else args.epochs
    
    logger.info(f"ğŸ”§ íŠœë‹ ëª¨ë“œ: {args.mode}")
    logger.info(f"ğŸ“Š ì—í­ ìˆ˜: {epochs}")
    
    # ì„¤ì • ëª©ë¡ ìƒì„±
    if args.mode == 'recommended':
        configs = RECOMMENDED_CONFIGS
    elif args.mode == 'grid':
        configs = generate_grid_configs(args.max_experiments)
    else:
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {args.mode}")
        sys.exit(1)
    
    # wandb ì„¤ì • (ëª…ì‹œì  í”Œë˜ê·¸ê°€ ì—†ìœ¼ë©´ configë¥¼ ë”°ë¦„)
    if args.use_wandb:
        use_wandb = True
    elif args.no_wandb:
        use_wandb = False
    else:
        from utils.config_manager import ConfigManager
        config_manager = ConfigManager(args.config)
        use_wandb = config_manager.get_config().get('logging', {}).get('use_wandb', False)
    
    # íŠœë‹ ì‹¤í–‰
    results_df = run_tuning(
        base_config_path=args.config,
        output_dir=args.output_dir,
        configs=configs,
        epochs=epochs,
        use_wandb=use_wandb
    )
    
    logger.info("\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")


if __name__ == "__main__":
    main()
